{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluating_roman_urdu_models_sa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGntJQIYZJ-C",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating Roman-Urdu Models through Sentiment Analysis\n",
        "\n",
        "This notebook was written to evaluate the Roman-Urdu models through a sentiment analysis task. This was performed on a labelled tweets dataset.\n",
        "\n",
        "References\n",
        "1. https://github.com/tthustla/twitter_sentiment_analysis_part11/blob/master/Capstone_part11.ipynb\n",
        "2. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsocKzKkX2MW",
        "colab_type": "text"
      },
      "source": [
        "#### Hiding warnings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UJVoGLwPYVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U_67by_X4qD",
        "colab_type": "text"
      },
      "source": [
        "#### Colab-specific statements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQAWBdHkPbEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    !pip install silence-tensorflow\n",
        "    print()\n",
        "    \n",
        "    drive.mount('/content/drive/')\n",
        "\n",
        "    base = '/content/drive/My Drive/Shared/FYP/'\n",
        "except:\n",
        "    base = 'C:/Users/Ali/Google Drive/Shared/FYP/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsHFuy0_X9oq",
        "colab_type": "text"
      },
      "source": [
        "## Loading Models\n",
        "\n",
        "#### Defining paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YzElHXPPkUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import silence_tensorflow\n",
        "import tensorflow\n",
        "\n",
        "word2vec_cbow_path = os.path.join(base, 'Models/Word2Vec/Ours/word2vec_roman_cbow_500')\n",
        "word2vec_sg_path = os.path.join(base, 'Models/Word2Vec/Ours/word2vec_roman_sg_500')\n",
        "\n",
        "glove_path =  os.path.join(base, 'Models/GloVe/glove_roman_500.txt')\n",
        "\n",
        "fasttext_cbow_path =  os.path.join(base, 'Models/fastText/roman_cbow/fasttext_roman_cbow_500')\n",
        "fasttext_sg_path =  os.path.join(base, 'Models/fastText/roman_sg/fasttext_roman_sg_500')\n",
        "\n",
        "tweets_path = os.path.join(base, 'Evaluation/roman_tweets.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT33X0SjYLjm",
        "colab_type": "text"
      },
      "source": [
        "#### Loading Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISih_gLEQFfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec_cbow = Word2Vec.load(word2vec_cbow_path)\n",
        "word2vec_cbow = word2vec_cbow.wv\n",
        "\n",
        "word2vec_sg = Word2Vec.load(word2vec_sg_path)\n",
        "word2vec_sg = word2vec_sg.wv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Wf4Mt1YON8",
        "colab_type": "text"
      },
      "source": [
        "#### GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzCYQ9o9QeOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "glove = KeyedVectors.load_word2vec_format(glove_path, binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-ogouu4YQC2",
        "colab_type": "text"
      },
      "source": [
        "#### Loading fastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYdb6om0Qe6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "fasttext_cbow = FastText.load(fasttext_cbow_path)\n",
        "fasttext_cbow = fasttext_cbow.wv\n",
        "\n",
        "fasttext_sg = FastText.load(fasttext_sg_path)\n",
        "fasttext_sg = fasttext_sg.wv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YemdiuzyYWRo",
        "colab_type": "text"
      },
      "source": [
        "## Loading and processing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id2Wf8mtrW1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_dataset(data_df):\n",
        "    \"\"\" Removes numbers and emojis from each tweet and labels positive tweets as 1 and negative ones as 0\"\"\"\n",
        "    cleaned = []\n",
        "\n",
        "    for row in data_df.values:\n",
        "        if isinstance(row[0], str) == True:\n",
        "            tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", row[0]).split()) \n",
        "            tweet = tweet.lower()\n",
        "            \n",
        "            label = 0\n",
        "            if row[1] == 'Positive':\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "                \n",
        "            cleaned.append([tweet, label])\n",
        "\n",
        "    return cleaned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQmMdIw65a_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df = pd.read_csv(tweets_path, header=None)\n",
        "cleaned_dataset = clean_dataset(data_df)\n",
        "dataset = pd.DataFrame(cleaned_dataset)\n",
        "dataset = dataset.sample(frac=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Wj3hQv2DGK",
        "colab_type": "text"
      },
      "source": [
        "## Preparing train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_HQjR6oBXCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = dataset[:16000][0].tolist()\n",
        "train_y = dataset[:16000][1].tolist()\n",
        "\n",
        "test_x = dataset[16000:][0].tolist()\n",
        "test_y = dataset[16000:][1].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzPEynE7-TVW",
        "colab_type": "text"
      },
      "source": [
        "## Building the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBPy6uUe-bAH",
        "colab_type": "text"
      },
      "source": [
        "#### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TPLNmw7GWIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def get_embedding_indexes(emb_model):\n",
        "    \"\"\" Returns a dictionary where each word in the trained word embedding model is mapped to an index\"\"\"\n",
        "    embeddings_index = {}\n",
        "\n",
        "    for word in emb_model.vocab.keys():\n",
        "            embeddings_index[word] = (emb_model.wv[word])\n",
        "    print('Found %s word vectors.' % len(embeddings_index))\n",
        "    \n",
        "    return embeddings_index\n",
        "\n",
        "def get_word_counts():\n",
        "    \"\"\" Returns the total number of words unique words present in the training data\"\"\"\n",
        "    word_counts = {}\n",
        "\n",
        "    for line in train_x:\n",
        "        tokens = line.split()\n",
        "        \n",
        "        for word in tokens:\n",
        "            if word in word_counts:\n",
        "                count = word_counts[word]\n",
        "                count += 1\n",
        "                word_counts[word] = count\n",
        "            else:\n",
        "                word_counts[word] = 0\n",
        "\n",
        "    return len(word_counts)\n",
        "\n",
        "def generate_embedding_matrix(tokenizer, embeddings_index, word_counts):\n",
        "    \"\"\" Returns a matrix of size nwords X embedding size, where each row is a word vector of a word in the train_set\"\"\"\n",
        "    embedding_matrix = np.zeros((word_counts, 500))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i >= word_counts:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return embedding_matrix\n",
        "\n",
        "def train_model(embedding_matrix, word_counts, x_train_seq, train_y):\n",
        "    \"\"\" Feeds the embedding matrix to the embedding layer as initial weights and starts training a NN on the sentiment analysis task\"\"\"\n",
        "    model = Sequential()\n",
        "    e = Embedding(word_counts, 500, weights=[embedding_matrix], input_length=320, trainable=True)\n",
        "    model.add(e)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(x_train_seq, train_y, epochs=5, batch_size=32, verbose=2)\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_embedding_model(emb_model, name, train_x, train_y, test_x, test_y):\n",
        "    \"\"\" Calls all the functions above sequentially and prints out the evaluation scores of all the models trained on the same dataset\"\"\"\n",
        "    tokenizer = Tokenizer(num_words=len(emb_model.wv.vocab))\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "    sequences = tokenizer.texts_to_sequences(train_x)\n",
        "\n",
        "    x_train_seq = pad_sequences(sequences, maxlen=320)\n",
        "\n",
        "    embeddings_index = get_embedding_indexes(emb_model)\n",
        "\n",
        "    word_counts = get_word_counts()\n",
        "\n",
        "    embedding_matrix = generate_embedding_matrix(tokenizer, embeddings_index, word_counts)\n",
        "    \n",
        "    model = train_model(embedding_matrix, word_counts, x_train_seq, train_y)\n",
        "    model.save(name + '.h5')\n",
        "\n",
        "    sequences_test = tokenizer.texts_to_sequences(test_x)\n",
        "    x_test_seq = pad_sequences(sequences_test, maxlen=320)   \n",
        "\n",
        "    print(\"{} accuracy: \".format(name))\n",
        "    print (model.evaluate(x=x_test_seq, y=test_y))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da-2cJUZ-pu5",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TePebBDTEu6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [word2vec_sg, word2vec_cbow, glove, fasttext_sg, fasttext_cbow]\n",
        "names = ['Word2Vec (SG)', 'Word2Vec (CBOW)', 'GloVe', 'fastText (SG)', 'fastText (CBOW)']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcGTLsvavRIq",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "for i in range(len(models)):\n",
        "    train_embedding_model(models[i], names[i], train_x, train_y, test_x, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}