{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluating_roman_urdu_models_sa_elmo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGntJQIYZJ-C",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating Roman-Urdu Models through Sentiment Analysis\n",
        "\n",
        "This notebook was written to evaluate the Roman-Urdu models through a sentiment analysis task. This was performed on a labelled tweets dataset.\n",
        "\n",
        "References\n",
        "1. https://github.com/tthustla/twitter_sentiment_analysis_part11/blob/master/Capstone_part11.ipynb\n",
        "2. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsocKzKkX2MW",
        "colab_type": "text"
      },
      "source": [
        "#### Hiding warnings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UJVoGLwPYVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U_67by_X4qD",
        "colab_type": "text"
      },
      "source": [
        "#### Colab-specific statements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQAWBdHkPbEI",
        "colab_type": "code",
        "outputId": "368eb30b-a3a9-43c8-91a5-e1b61b89792c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsHFuy0_X9oq",
        "colab_type": "text"
      },
      "source": [
        "## Loading Models\n",
        "\n",
        "#### Defining paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YzElHXPPkUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow\n",
        "\n",
        "base = '/content/drive/My Drive/FYP/'\n",
        "\n",
        "elmo_path = '/content/drive/My Drive/FYP/Models/ELMo/roman-urdu/embeddings.txt'\n",
        "\n",
        "tweets_path = '/content/drive/My Drive/FYP/Evaluation/roman-urdu_tweets.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzCYQ9o9QeOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "elmo = KeyedVectors.load_word2vec_format(elmo_path, binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YemdiuzyYWRo",
        "colab_type": "text"
      },
      "source": [
        "## Loading and processing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id2Wf8mtrW1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_dataset(data_df):\n",
        "    \"\"\" Removes numbers and emojis from each tweet and labels positive tweets as 1 and negative ones as 0\"\"\"\n",
        "    cleaned = []\n",
        "\n",
        "    for row in data_df.values:\n",
        "        if isinstance(row[0], str) == True:\n",
        "            tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", row[0]).split()) \n",
        "            tweet = tweet.lower()\n",
        "            \n",
        "            label = 0\n",
        "            if row[1] == 'Positive':\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "                \n",
        "            cleaned.append([tweet, label])\n",
        "\n",
        "    return cleaned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQmMdIw65a_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df = pd.read_csv(tweets_path, header=None)\n",
        "cleaned_dataset = clean_dataset(data_df)\n",
        "dataset = pd.DataFrame(cleaned_dataset)\n",
        "dataset = dataset.sample(frac=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Wj3hQv2DGK",
        "colab_type": "text"
      },
      "source": [
        "## Preparing train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_HQjR6oBXCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = dataset[:16000][0].tolist()\n",
        "train_y = dataset[:16000][1].tolist()\n",
        "\n",
        "test_x = dataset[16000:][0].tolist()\n",
        "test_y = dataset[16000:][1].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzPEynE7-TVW",
        "colab_type": "text"
      },
      "source": [
        "## Building the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBPy6uUe-bAH",
        "colab_type": "text"
      },
      "source": [
        "#### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TPLNmw7GWIr",
        "colab_type": "code",
        "outputId": "29985259-9aa5-421f-a25b-544f06da2e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def get_embedding_indexes(emb_model):\n",
        "    \"\"\" Returns a dictionary where each word in the trained word embedding model is mapped to an index\"\"\"\n",
        "    embeddings_index = {}\n",
        "\n",
        "    for word in emb_model.vocab.keys():\n",
        "            embeddings_index[word] = (emb_model.wv[word])\n",
        "    print('Found %s word vectors.' % len(embeddings_index))\n",
        "    \n",
        "    return embeddings_index\n",
        "\n",
        "def get_word_counts():\n",
        "    \"\"\" Returns the total number of words unique words present in the training data\"\"\"\n",
        "    word_counts = {}\n",
        "\n",
        "    for line in train_x:\n",
        "        tokens = line.split()\n",
        "        \n",
        "        for word in tokens:\n",
        "            if word in word_counts:\n",
        "                count = word_counts[word]\n",
        "                count += 1\n",
        "                word_counts[word] = count\n",
        "            else:\n",
        "                word_counts[word] = 0\n",
        "\n",
        "    return len(word_counts)\n",
        "\n",
        "def generate_embedding_matrix(tokenizer, embeddings_index, word_counts):\n",
        "    \"\"\" Returns a matrix of size nwords X embedding size, where each row is a word vector of a word in the train_set\"\"\"\n",
        "    embedding_matrix = np.zeros((word_counts, 500))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i >= word_counts:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return embedding_matrix\n",
        "\n",
        "def train_model(embedding_matrix, word_counts, x_train_seq, train_y):\n",
        "    \"\"\" Feeds the embedding matrix to the embedding layer as initial weights and starts training a NN on the sentiment analysis task\"\"\"\n",
        "    model = Sequential()\n",
        "    e = Embedding(word_counts, 500, weights=[embedding_matrix], input_length=320, trainable=True)\n",
        "    model.add(e)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(x_train_seq, train_y, epochs=5, batch_size=32, verbose=2)\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_embedding_model(emb_model, name, train_x, train_y, test_x, test_y):\n",
        "    \"\"\" Calls all the functions above sequentially and prints out the evaluation scores of all the models trained on the same dataset\"\"\"\n",
        "    tokenizer = Tokenizer(num_words=len(emb_model.wv.vocab))\n",
        "    tokenizer.fit_on_texts(train_x)\n",
        "    sequences = tokenizer.texts_to_sequences(train_x)\n",
        "\n",
        "    x_train_seq = pad_sequences(sequences, maxlen=320)\n",
        "\n",
        "    embeddings_index = get_embedding_indexes(emb_model)\n",
        "\n",
        "    word_counts = get_word_counts()\n",
        "\n",
        "    embedding_matrix = generate_embedding_matrix(tokenizer, embeddings_index, word_counts)\n",
        "    \n",
        "    model = train_model(embedding_matrix, word_counts, x_train_seq, train_y)\n",
        "    model.save(name + '.h5')\n",
        "\n",
        "    sequences_test = tokenizer.texts_to_sequences(test_x)\n",
        "    x_test_seq = pad_sequences(sequences_test, maxlen=320)   \n",
        "\n",
        "    print(\"{} accuracy: \".format(name))\n",
        "    print (model.evaluate(x=x_test_seq, y=test_y))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da-2cJUZ-pu5",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TePebBDTEu6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [elmo]\n",
        "names = ['Elmo']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcGTLsvavRIq",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "7f1b10ce-0982-4267-ac9b-6a1730cb8b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "for i in range(len(models)):\n",
        "    train_embedding_model(models[i], names[i], train_x, train_y, test_x, test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 37172 word vectors.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/5\n",
            " - 24s - loss: 0.5740 - acc: 0.7222\n",
            "Epoch 2/5\n",
            " - 21s - loss: 0.4214 - acc: 0.8105\n",
            "Epoch 3/5\n",
            " - 21s - loss: 0.2212 - acc: 0.9109\n",
            "Epoch 4/5\n",
            " - 21s - loss: 0.0883 - acc: 0.9717\n",
            "Epoch 5/5\n",
            " - 21s - loss: 0.0361 - acc: 0.9902\n",
            "Elmo accuracy: \n",
            "4228/4228 [==============================] - 0s 105us/step\n",
            "[1.1395248333688266, 0.7594607379375591]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}