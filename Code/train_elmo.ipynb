{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Siy-5Ur84e2A"
   },
   "source": [
    "# Train ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74eCoOhRuEYl"
   },
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Rh4SF0YsWkP"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#MODEL_LANGUAGE = 'urdu'\n",
    "MODEL_LANGUAGE = 'roman_urdu'\n",
    "\n",
    "!git clone https://github.com/allenai/bilm-tf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUE41bDrnsn_"
   },
   "outputs": [],
   "source": [
    "cd /content/bilm-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbc9QODis3eO"
   },
   "outputs": [],
   "source": [
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U73_O44rUp6S"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6QsZQYL1cAr"
   },
   "outputs": [],
   "source": [
    "#!rm \"/content/bilm-tf/bilm/data.py\"\n",
    "!rm \"/content/bilm-tf/bin/train_elmo.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5oLyIop3BXk"
   },
   "outputs": [],
   "source": [
    "#!cp \"/content/drive/My Drive/FYP/Models/ELMo/roman_urdu/data.py\" \"/content/bilm-tf/bilm/data.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZshF7T6q_sx"
   },
   "source": [
    "## Preparing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UyLXH5TSuWTj"
   },
   "source": [
    "### Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UCroFIwUuib"
   },
   "outputs": [],
   "source": [
    "def load_corpus(corpus):\n",
    "    lines = []\n",
    "\n",
    "    with open(corpus) as input_file:\n",
    "        lines = input_file.readlines()\n",
    "\n",
    "    return lines\n",
    "\n",
    "corpus = load_corpus('/content/drive/My Drive/FYP/Corpora/Training/' + MODEL_LANGUAGE + '-filtered.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UmTYGruyrFZ_"
   },
   "source": [
    "### Splitting the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "haBGiQqdfahc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split_corpus(corpus, lines_per_file, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for i in range(0, len(corpus), lines_per_file):\n",
    "        text = \"\\n\".join(corpus[i: i + lines_per_file])\n",
    "        \n",
    "        with open(save_dir + str(i) + \".txt\", \"w\", encoding='utf-8', errors='ignore') as fp:\n",
    "            fp.write(text)\n",
    "\n",
    "split_corpus(corpus, 1000, \"/content/swb/train/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMKtf3EcrK8l"
   },
   "source": [
    "### Creating the vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6FjifE6kfVJ"
   },
   "outputs": [],
   "source": [
    "def get_tokens_dict(corpus):\n",
    "    tokens = {}\n",
    "\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            if word in tokens:\n",
    "                tokens[word] += 1\n",
    "            else:\n",
    "                tokens[word] = 1\n",
    "\n",
    "    return sorted(tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def write_vocab_file(filename, tokens):\n",
    "    with open(filename, 'w', encoding='utf-8', errors='ignore') as vocab_file:\n",
    "        vocab_file.write(\"<S>\\n</S>\\n<UNK>\\n\")\n",
    "\n",
    "        for token in tokens:\n",
    "            vocab_file.write(token[0] + \"\\n\")\n",
    "\n",
    "tokens_dict = get_tokens_dict(corpus)\n",
    "write_vocab_file('vocab.txt', tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swQLDnbJKIVx"
   },
   "outputs": [],
   "source": [
    "if MODEL_LANGUAGE == 'urdu':\n",
    "    !cp /content/vocab.txt \"/content/drive/My Drive/FYP/Models/ELMo/urdu/vocab.txt\"\n",
    "else:\n",
    "    !cp /content/vocab.txt \"/content/drive/My Drive/FYP/Models/ELMo/roman_urdu/vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2cM5mGHrh7Q"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgtdIeTDivX4"
   },
   "outputs": [],
   "source": [
    "# Delete the file from the cloned repo\n",
    "!rm \"/content/bilm-tf/bin/train_elmo.py\"\n",
    "\n",
    "# Copy the modified file to the repo directory\n",
    "if MODEL_LANGUAGE == 'urdu':\n",
    "    !cp \"/content/drive/My Drive/FYP/Models/ELMo/urdu/train_elmo.py\" \"/content/bilm-tf/bin/train_elmo.py\"\n",
    "else:\n",
    "    !cp \"/content/drive/My Drive/FYP/Models/ELMo/roman_urdu/train_elmo.py\" \"/content/bilm-tf/bin/train_elmo.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yj-MohKtuBjv"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/content/drive/My Drive/FYP/Models/ELMo/\" + MODEL_LANGUAGE + \"/checkpoint\"):\n",
    "    os.makedirs(\"/content/drive/My Drive/FYP/Models/ELMo/\" + MODEL_LANGUAGE + \"/checkpoint\")\n",
    "\n",
    "if MODEL_LANGUAGE == 'urdu':\n",
    "    json_file = '{\"lstm\": {\"use_skip_connections\": true, \"projection_dim\": 500, \"cell_clip\": 3, \"proj_clip\": 3, \"dim\": 1024, \"n_layers\": 2}, \"char_cnn\": {\"activation\": \"relu\", \"filters\": [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]], \"n_highway\": 1, \"embedding\": {\"dim\": 16}, \"n_characters\": 261, \"max_characters_per_token\": 45}}'\n",
    "else:\n",
    "    json_file = '{\"lstm\": {\"use_skip_connections\": true, \"projection_dim\": 500, \"cell_clip\": 3, \"proj_clip\": 3, \"dim\": 1024, \"n_layers\": 2}, \"char_cnn\": {\"activation\": \"relu\", \"filters\": [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]], \"n_highway\": 1, \"embedding\": {\"dim\": 16}, \"n_characters\": 261, \"max_characters_per_token\": 15}}'\n",
    "\n",
    "with open(\"/content/drive/My Drive/FYP/Models/ELMo/\" + MODEL_LANGUAGE + \"/checkpoint/options.json\", \"w\") as fp:\n",
    "    fp.write(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJrBB1mcmKm4"
   },
   "outputs": [],
   "source": [
    "if MODEL_LANGUAGE == 'urdu': \n",
    "    !python bilm-tf/bin/train_elmo.py \\\n",
    "        --train_prefix='/content/swb/train/*' \\\n",
    "        --vocab_file \"/content/drive/My Drive/FYP/Models/ELMo/urdu/vocab.txt\" \\\n",
    "        --save_dir '/content/drive/My Drive/FYP/Models/ELMo/urdu/checkpoint/'\n",
    "else:\n",
    "    !python bilm-tf/bin/train_elmo.py \\\n",
    "        --train_prefix='/content/swb/train/*' \\\n",
    "        --vocab_file \"/content/drive/My Drive/FYP/Models/ELMo/roman_urdu/vocab.txt\" \\\n",
    "        --save_dir '/content/drive/My Drive/FYP/Models/ELMo/roman_urdu/checkpoint/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OstNeJRdTOh7"
   },
   "source": [
    "### Converting the TensorFlow checkpoint to hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YroMpoycTYLm"
   },
   "outputs": [],
   "source": [
    "if MODEL_LANGUAGE == 'urdu':\n",
    "    !python bilm-tf/bin/dump_weights.py \\\n",
    "        --save_dir '/content/drive/My Drive/FYP/Models/ELMo/urdu/checkpoint/' \\\n",
    "        --outfile '/content/drive/My Drive/FYP/Models/ELMo/urdu/weights.hdf5'\n",
    "else:\n",
    "    !python bilm-tf/bin/dump_weights.py \\\n",
    "        --save_dir '/content/drive/My Drive/FYP/Models/ELMo/roman_urdu/checkpoint/' \\\n",
    "        --outfile '/content/drive/My Drive/FYP/Models/ELMo/roman_urdu/weights.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwUgMOsOIV5K"
   },
   "source": [
    "## Extracting embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqTBg0zfuyJe"
   },
   "source": [
    "### Dumping the token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHzGvf-3IVbF"
   },
   "outputs": [],
   "source": [
    "from bilm.model import dump_token_embeddings\n",
    "\n",
    "dump_token_embeddings('/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/vocab.txt',\n",
    "                          '/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/options.json',\n",
    "                          '/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/weights.hdf5',\n",
    "                          '/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/embeddings.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NCbNpgeQu1o2"
   },
   "source": [
    "### Saving context-independent embeddings in Word2Vec format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yBxukt_JQBK"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "embeddings_file = '/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/embeddings.txt'\n",
    "vocab_file = '/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/vocab.txt'\n",
    "\n",
    "embeddings_file = h5py.File(embeddings_file, 'r')\n",
    "embeddings = list(embeddings_file['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwW22VyOsYFi"
   },
   "outputs": [],
   "source": [
    "def create_word_embeddings_dict(words, embeddings):\n",
    "    word_embeddings = {}\n",
    "    \n",
    "    with open(words, encoding='utf-8', errors='ignore') as words_file: \n",
    "        lines = 0\n",
    "\n",
    "        for word, vector in zip(words_file, embeddings):\n",
    "            if word not in ['<S>\\n', '</S>\\n', '<UNK>\\n']:\n",
    "                word = word.rstrip()\n",
    "                word_embeddings[word] = vector\n",
    "                lines += 1\n",
    "        \n",
    "    return word_embeddings\n",
    "    \n",
    "word_embeddings = create_word_embeddings_dict(vocab_file, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qlJpB7PbsZcW"
   },
   "outputs": [],
   "source": [
    "def write_embeddings_file(output_file, word_vector_dict):\n",
    "    with open(output_file, 'w', encoding='utf-8', errors='ignore') as o_f:\n",
    "        o_f.write(str(len(word_embeddings.keys()) - 3) + \" 500\\n\")\n",
    "\n",
    "        for key in word_vector_dict:                        \n",
    "            line = \"\"\n",
    "            line += key\n",
    "\n",
    "            for dim in word_vector_dict[key]:\n",
    "                line += \" \"\n",
    "                line += str(dim)\n",
    "            o_f.write(line + \"\\n\")\n",
    "\n",
    "write_embeddings_file('/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/embeddings.txt', word_embeddings)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "train_elmo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
