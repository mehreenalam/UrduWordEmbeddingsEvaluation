{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_elmo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Siy-5Ur84e2A",
        "colab_type": "text"
      },
      "source": [
        "#Training ELMo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74eCoOhRuEYl",
        "colab_type": "text"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rh4SF0YsWkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#MODEL_LANGUAGE = 'urdu'\n",
        "MODEL_LANGUAGE = 'roman-urdu'\n",
        "\n",
        "!git clone https://github.com/allenai/bilm-tf.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUE41bDrnsn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/bilm-tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbc9QODis3eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U73_O44rUp6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6QsZQYL1cAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm \"/content/bilm-tf/bilm/data.py\"\n",
        "!rm \"/content/bilm-tf/bin/train_elmo.py\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5oLyIop3BXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/FYP/Models/ELMo/roman-urdu/train_elmo.py\" \"/content/bilm-tf/bin/train_elmo.py\"\n",
        "#!cp \"/content/drive/My Drive/FYP/Models/ELMo/roman-urdu/data.py\" \"/content/bilm-tf/bilm/data.py\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZshF7T6q_sx",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyLXH5TSuWTj",
        "colab_type": "text"
      },
      "source": [
        "### Loading the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UCroFIwUuib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_corpus(corpus):\n",
        "    lines = []\n",
        "\n",
        "    with open(corpus) as input_file:\n",
        "        lines = input_file.readlines()\n",
        "\n",
        "    return lines\n",
        "\n",
        "corpus = load_corpus('/content/drive/My Drive/FYP/Corpora/Training/' + MODEL_LANGUAGE + '_filtered.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmTYGruyrFZ_",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haBGiQqdfahc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def split_corpus(corpus, lines_per_file, save_dir):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    for i in range(0, len(corpus), lines_per_file):\n",
        "        text = \"\\n\".join(corpus[i: i + lines_per_file])\n",
        "        \n",
        "        with open(save_dir + str(i) + \".txt\", \"w\", encoding='utf-8', errors='ignore') as fp:\n",
        "            fp.write(text)\n",
        "\n",
        "split_corpus(corpus, 1000, \"/content/swb/train/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMKtf3EcrK8l",
        "colab_type": "text"
      },
      "source": [
        "### Creating the vocab file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6FjifE6kfVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokens_dict(corpus):\n",
        "    tokens = {}\n",
        "\n",
        "    for sentence in corpus:\n",
        "        words = sentence.split()\n",
        "        for word in words:\n",
        "            if word in tokens:\n",
        "                tokens[word] += 1\n",
        "            else:\n",
        "                tokens[word] = 1\n",
        "\n",
        "    return sorted(tokens.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def write_vocab_file(filename, tokens):\n",
        "    with open(filename, 'w', encoding='utf-8', errors='ignore') as vocab_file:\n",
        "        vocab_file.write(\"<S>\\n</S>\\n<UNK>\\n\")\n",
        "\n",
        "        for token in tokens:\n",
        "            vocab_file.write(token[0] + \"\\n\")\n",
        "\n",
        "tokens_dict = get_tokens_dict(corpus)\n",
        "write_vocab_file('vocab.txt', tokens_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swQLDnbJKIVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MODEL_LANGUAGE == 'urdu':\n",
        "    !cp /content/vocab.txt \"/content/drive/My Drive/FYP/Models/ELMo/urdu/vocab.txt\"\n",
        "else:\n",
        "    !cp /content/vocab.txt \"/content/drive/My Drive/FYP/Models/ELMo/roman-urdu/vocab.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2cM5mGHrh7Q",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgtdIeTDivX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete the file from the cloned repo\n",
        "!rm \"/content/bilm-tf/bin/train_elmo.py\"\n",
        "\n",
        "# Copy the modified file to the repo directory\n",
        "if MODEL_LANGUAGE == 'urdu':\n",
        "    !cp \"/content/drive/My Drive/FYP/Models/ELMo/urdu/train_elmo.py\" \"/content/bilm-tf/bin/train_elmo.py\"\n",
        "else:\n",
        "    !cp \"/content/drive/My Drive/FYP/Models/ELMo/roman-urdu/train_elmo.py\" \"/content/bilm-tf/bin/train_elmo.py\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj-MohKtuBjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"/content/drive/My Drive/FYP/Models/ELMo/\" + MODEL_LANGUAGE + \"/checkpoint\"):\n",
        "    os.makedirs(\"/content/drive/My Drive/FYP/Models/ELMo/\" + MODEL_LANGUAGE + \"/checkpoint\")\n",
        "\n",
        "if MODEL_LANGUAGE == 'urdu':\n",
        "    json_file = '{\"lstm\": {\"use_skip_connections\": true, \"projection_dim\": 500, \"cell_clip\": 3, \"proj_clip\": 3, \"dim\": 1024, \"n_layers\": 2}, \"char_cnn\": {\"activation\": \"relu\", \"filters\": [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]], \"n_highway\": 1, \"embedding\": {\"dim\": 16}, \"n_characters\": 261, \"max_characters_per_token\": 45}}'\n",
        "else:\n",
        "    json_file = '{\"lstm\": {\"use_skip_connections\": true, \"projection_dim\": 500, \"cell_clip\": 3, \"proj_clip\": 3, \"dim\": 1024, \"n_layers\": 2}, \"char_cnn\": {\"activation\": \"relu\", \"filters\": [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]], \"n_highway\": 1, \"embedding\": {\"dim\": 16}, \"n_characters\": 261, \"max_characters_per_token\": 15}}'\n",
        "\n",
        "with open(\"/content/drive/My Drive/FYP/Models/ELMo/\" + MODEL_LANGUAGE + \"/checkpoint/options.json\", \"w\") as fp:\n",
        "    fp.write(json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJrBB1mcmKm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MODEL_LANGUAGE == 'urdu': \n",
        "    !python bilm-tf/bin/train_elmo.py \\\n",
        "        --train_prefix='/content/swb/train/*' \\\n",
        "        --vocab_file \"/content/drive/My Drive/FYP/Models/ELMo/urdu/vocab.txt\" \\\n",
        "        --save_dir '/content/drive/My Drive/FYP/Models/ELMo/urdu/checkpoint/'\n",
        "else:\n",
        "    !python bilm-tf/bin/train_elmo.py \\\n",
        "        --train_prefix='/content/swb/train/*' \\\n",
        "        --vocab_file \"/content/drive/My Drive/FYP/Models/ELMo/roman-urdu/vocab.txt\" \\\n",
        "        --save_dir '/content/drive/My Drive/FYP/Models/ELMo/roman-urdu/checkpoint/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OstNeJRdTOh7",
        "colab_type": "text"
      },
      "source": [
        "### Converting the TensorFlow checkpoint to hdf5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YroMpoycTYLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MODEL_LANGUAGE == 'urdu':\n",
        "    !python bilm-tf/bin/dump_weights.py \\\n",
        "        --save_dir '/content/drive/My Drive/FYP/Models/ELMo/urdu/checkpoint/' \\\n",
        "        --outfile '/content/drive/My Drive/FYP/Models/ELMo/urdu/weights.hdf5'\n",
        "else:\n",
        "    !python bilm-tf/bin/dump_weights.py \\\n",
        "        --save_dir '/content/drive/My Drive/FYP/Models/ELMo/roman-urdu/checkpoint/' \\\n",
        "        --outfile '/content/drive/My Drive/FYP/Models/ELMo/roman-urdu/weights.hdf5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwUgMOsOIV5K",
        "colab_type": "text"
      },
      "source": [
        "## Extracting embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqTBg0zfuyJe",
        "colab_type": "text"
      },
      "source": [
        "### Dumping the token embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHzGvf-3IVbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bilm.model import dump_token_embeddings\n",
        "\n",
        "dump_token_embeddings('/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/vocab.txt',\n",
        "                          '/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/options.json',\n",
        "                          '/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/weights.hdf5',\n",
        "                          '/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/embeddings.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCbNpgeQu1o2",
        "colab_type": "text"
      },
      "source": [
        "### Saving context-independent embeddings in Word2Vec format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yBxukt_JQBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "\n",
        "embeddings_file = '/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/embeddings.txt'\n",
        "vocab_file = '/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/vocab.txt'\n",
        "\n",
        "embeddings_file = h5py.File(embeddings_file, 'r')\n",
        "embeddings = list(embeddings_file['embedding'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwW22VyOsYFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_word_embeddings_dict(words, embeddings):\n",
        "    word_embeddings = {}\n",
        "    \n",
        "    with open(words, encoding='utf-8', errors='ignore') as words_file: \n",
        "        lines = 0\n",
        "\n",
        "        for word, vector in zip(words_file, embeddings):\n",
        "            if word not in ['<S>\\n', '</S>\\n', '<UNK>\\n']:\n",
        "                word = word.rstrip()\n",
        "                word_embeddings[word] = vector\n",
        "                lines += 1\n",
        "        \n",
        "    return word_embeddings\n",
        "    \n",
        "word_embeddings = create_word_embeddings_dict(vocab_file, embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlJpB7PbsZcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_embeddings_file(output_file, word_vector_dict):\n",
        "    with open(output_file, 'w', encoding='utf-8', errors='ignore') as o_f:\n",
        "        o_f.write(str(len(word_embeddings.keys()) - 3) + \" 500\\n\")\n",
        "\n",
        "        for key in word_vector_dict:                        \n",
        "            line = \"\"\n",
        "            line += key\n",
        "\n",
        "            for dim in word_vector_dict[key]:\n",
        "                line += \" \"\n",
        "                line += str(dim)\n",
        "            o_f.write(line + \"\\n\")\n",
        "\n",
        "write_embeddings_file('/content/drive/My Drive/FYP/Models/ELMo/' + MODEL_LANGUAGE + '/embeddings.txt', word_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}