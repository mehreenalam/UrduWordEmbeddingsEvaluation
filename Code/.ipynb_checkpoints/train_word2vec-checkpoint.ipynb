{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_rGQISuqDoY"
   },
   "source": [
    "# Training word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7naMCy9gigpd"
   },
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "filename = \"roman_processed.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VdY6l3erqUq7"
   },
   "source": [
    "## Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5r0G7DuMjQKz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "lines = LineSentence(path + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zlmrDt46qZ48"
   },
   "source": [
    "## Training a word2vec model\n",
    "\n",
    "### Usage\n",
    "Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None,\n",
    "         sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1,\n",
    "         hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000,\n",
    "         compute_loss=False, callbacks=(), max_final_vocab=None)\n",
    "\n",
    "### Important parameters\n",
    "1. **sentences** (iterable of iterables, optional) – The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network.\n",
    "2. **corpus_file** (str, optional) – Path to a corpus file in LineSentence format. You may use this argument instead of sentences to get performance boost. Only one of sentences or corpus_file arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
    "3. **size** (int, optional) – Dimensionality of the word vectors.\n",
    "4. **window** (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "5. **min_count** (int, optional) – Ignores all words with total frequency lower than this.\n",
    "6. **workers** (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "7. **sg** ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "8. **hs** ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "9. **negative** (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "10. **ns_exponent** (float, optional) – The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more than high-frequency words.\n",
    "11. **cbow_mean** ({0, 1}, optional) – If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
    "12. **alpha** (float, optional) – The initial learning rate.\n",
    "13. **min_alpha** (float, optional) – Learning rate will linearly drop to min_alpha as training progresses.\n",
    "14. **seed** (int, optional) – Seed for the random number generator.\n",
    "15. **max_vocab_size** (int, optional) – Limits the RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Set to None for no limit.\n",
    "16. **max_final_vocab** (int, optional) – Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified min_count is more than the calculated min_count, the specified min_count will be used. Set to None if not required.\n",
    "17. **sample** (float, optional) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "18. **iter** (int, optional) – Number of iterations (epochs) over the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XufqWU_rk000"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "size = 500                               # Vector dimension\n",
    "window = 5                               # Context window size\n",
    "min_count = 1                            # Cut off frequency\n",
    "sg = 1                                   # 0 for CBOW, 1 for Skip-gram\n",
    "sample = 0.001                           # Default\n",
    "workers = multiprocessing.cpu_count()    # Number of cores to use\n",
    "\n",
    "model = Word2Vec(size=size, window=window, min_count=min_count, sg=sg, sample=sample, workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fHFPCWIoK_y"
   },
   "outputs": [],
   "source": [
    "def get_model_name():\n",
    "    model_name = 'word2vec'\n",
    "    \n",
    "    if 'roman' in filename:\n",
    "        model_name += '_roman'\n",
    "    else:\n",
    "        model_name += '_urdu'\n",
    "        \n",
    "    if sg == 0:\n",
    "        model_name += '_cbow'\n",
    "    else:\n",
    "        model_name += '_sg'\n",
    "        \n",
    "    model_name += '_' + str(size)\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKMlencQo5Ko"
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "model.build_vocab(lines, progress_per=1000000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcRW4JWGpIsg"
   },
   "outputs": [],
   "source": [
    "model_name = get_model_name()\n",
    "print(\"About to train {}\".format(model_name))\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "model.train(lines, total_examples=model.corpus_count, epochs=5, report_delay=10)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9LGww4IQOUZ"
   },
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec_gensim_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
