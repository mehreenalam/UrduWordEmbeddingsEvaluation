{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fasttext_gensim_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5_rGQISuqDoY"
      },
      "source": [
        "# Training fastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpJrVxdvhZU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/drive/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7naMCy9gigpd",
        "colab": {}
      },
      "source": [
        "root = \"/drive/My Drive\"\n",
        "path = \"/Shared/FYP/Data/\"\n",
        "filename = \"roman_processed.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VdY6l3erqUq7"
      },
      "source": [
        "## Loading the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5r0G7DuMjQKz",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import logging\n",
        "from gensim.models.word2vec import LineSentence\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
        "\n",
        "lines = LineSentence(root + path + filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zlmrDt46qZ48"
      },
      "source": [
        "## Training a fastText model\n",
        "\n",
        "### Usage\n",
        "FastText(sentences=None, corpus_file=None, sg=0, hs=0, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), compatible_hash=True)\n",
        "\n",
        "\n",
        "**Parameters:**\n",
        "1. **sentences** (iterable of list of str, optional) – Can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network. See BrownCorpus, Text8Corpus or LineSentence in word2vec module for such examples. If you don’t supply sentences, the model is left uninitialized – use if you plan to initialize it in some other way.\n",
        "2. **corpus_file** (str, optional) – Path to a corpus file in LineSentence format. You may use this argument instead of sentences to get performance boost. Only one of sentences or corpus_file arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
        "3. **min_count** (int, optional) – The model ignores all words with total frequency lower than this.\n",
        "4. **size** (int, optional) – Dimensionality of the word vectors.\n",
        "5. **window** (int, optional) – The maximum distance between the current and predicted word within a sentence.\n",
        "6. **workers** (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
        "7. **alpha** (float, optional) – The initial learning rate.\n",
        "8. **min_alpha** (float, optional) – Learning rate will linearly drop to min_alpha as training progresses.\n",
        "9. **sg** ({1, 0}, optional) – Training algorithm: skip-gram if sg=1, otherwise CBOW.\n",
        "10. **hs** ({1,0}, optional) – If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.\n",
        "11. **seed** (int, optional) – Seed for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). Note that for a fully deterministically-reproducible run, you must also limit the model to a single worker thread (workers=1), to eliminate ordering jitter from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED environment variable to control hash randomization).\n",
        "12. **max_vocab_size** (int, optional) – Limits the RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Set to None for no limit.\n",
        "13. **sample** (float, optional) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
        "14. **negative** (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
        "15. **ns_exponent** (float, optional) – The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper. More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that other values may perform better for recommendation applications.\n",
        "16. **cbow_mean** ({1,0}, optional) – If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
        "17. **hashfxn** (function, optional) – Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
        "18. **iter** (int, optional) – Number of iterations (epochs) over the corpus.\n",
        "19. **trim_rule** (function, optional) –\n",
        "\n",
        "    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary, be trimmed away, or handled using the default (discard if word count < min_count). Can be None (min_count will be used, look to keep_vocab_item()), or a callable that accepts parameters (word, count, min_count) and returns either gensim.utils.RULE_DISCARD, gensim.utils.RULE_KEEP or gensim.utils.RULE_DEFAULT. The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of themodel.\n",
        "\n",
        "    The input parameters are of the following types:\n",
        "            word (str) - the word we are examining\n",
        "            count (int) - the word’s frequency count in the corpus\n",
        "            min_count (int) - the minimum count threshold.\n",
        "\n",
        "20. **sorted_vocab** ({1,0}, optional) – If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
        "21. **batch_words** (int, optional) – Target size (in words) for batches of examples passed to worker threads (and thus cython routines).(Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
        "22. **min_n** (int, optional) – Minimum length of char n-grams to be used for training word representations.\n",
        "23. **max_n** (int, optional) – Max length of char ngrams to be used for training word representations. Set max_n to be lesser than min_n to avoid char ngrams being used.\n",
        "24. **word_ngrams** ({1,0}, optional) – If 1, uses enriches word vectors with subword(n-grams) information. If 0, this is equivalent to Word2Vec.\n",
        "25. **bucket** (int, optional) – Character ngrams are hashed into a fixed number of buckets, in order to limit the memory usage of the model. This option specifies the number of buckets used by the model.\n",
        "26. **callbacks** – List of callbacks that need to be executed/run at specific stages during training.\n",
        "27. **compatible_hash** (bool, optional) – By default, newer versions of Gensim’s FastText use a hash function that is 100% compatible with Facebook’s FastText. Older versions were not 100% compatible due to a bug. To use the older, incompatible hash function, set this to False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XufqWU_rk000",
        "colab": {}
      },
      "source": [
        "import multiprocessing\n",
        "from gensim.models import FastText\n",
        "\n",
        "size = 500                               # Vector dimension\n",
        "window = 5                               # Context window size\n",
        "min_count = 1                            # Cut off frequency\n",
        "sg = 0                                   # 0 for CBOW, 1 for Skip-gram\n",
        "workers = multiprocessing.cpu_count()    # Number of cores to use\n",
        "\n",
        "model = FastText(size=size, window=window, min_count=min_count, sg=sg, workers=workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fHFPCWIoK_y",
        "colab": {}
      },
      "source": [
        "def get_model_name():\n",
        "    model_name = 'fasttext'\n",
        "    \n",
        "    if 'roman' in filename:\n",
        "        model_name += '_roman'\n",
        "    else:\n",
        "        model_name += '_urdu'\n",
        "        \n",
        "    if sg == 0:\n",
        "        model_name += '_cbow'\n",
        "    else:\n",
        "        model_name += '_sg'\n",
        "        \n",
        "    model_name += '_' + str(size)\n",
        "    return model_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MKMlencQo5Ko",
        "colab": {}
      },
      "source": [
        "t = time.time()\n",
        "\n",
        "model.build_vocab(sentences=lines, progress_per=1000000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KcRW4JWGpIsg",
        "colab": {}
      },
      "source": [
        "model_name = get_model_name()\n",
        "print(\"About to train {}\".format(model_name))\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "model.train(sentences=lines, total_examples=model.corpus_count, epochs=5, report_delay=10)\n",
        "\n",
        "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i9LGww4IQOUZ"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkWmMwSthXqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}